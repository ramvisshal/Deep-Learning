{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
        "import numpy as np\n",
        "\n",
        "data = \"Deep learning is amazing. Deep learning builds intelligent systems.\"\n",
        "\n",
        "data = data.lower().replace(\".\", \"\").replace(\",\", \"\")\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "word_index = tokenizer.word_index\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "words = data.split()\n",
        "sequences = []\n",
        "for i in range(1, len(words)):\n",
        "    seq = words[:i+1]\n",
        "    sequences.append(' '.join(seq))\n",
        "\n",
        "encoded = tokenizer.texts_to_sequences(sequences)\n",
        "max_len = max(len(seq) for seq in encoded)\n",
        "\n",
        "X = np.array([seq[:-1] for seq in pad_sequences(encoded, maxlen=max_len)])\n",
        "y = to_categorical(\n",
        "    [seq[-1] for seq in pad_sequences(encoded, maxlen=max_len)],\n",
        "    num_classes=vocab_size\n",
        ")\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=10, input_length=max_len-1),\n",
        "    SimpleRNN(50),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "def generate_next_word(seed_text):\n",
        "    seed_text = seed_text.lower().replace(\".\", \"\").replace(\",\", \"\")\n",
        "    tokens = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "    tokens = pad_sequences([tokens], maxlen=max_len-1)\n",
        "    predicted_index = np.argmax(model.predict(tokens, verbose=0))\n",
        "    for word, index in word_index.items():\n",
        "        if index == predicted_index:\n",
        "            return word\n",
        "    return \"[unknown]\"\n",
        "\n",
        "seed = \"deep learning\"\n",
        "next_word = generate_next_word(seed)\n",
        "print(f\"Input: '{seed}' → Predicted next word: '{next_word}'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FU6uHCQezyVU",
        "outputId": "ffaf905e-ae67-4fbb-bb04-6f607e997402"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 'deep learning' → Predicted next word: 'is'\n"
          ]
        }
      ]
    }
  ]
}